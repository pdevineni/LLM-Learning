{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using Llamaindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup environment \n",
    "\n",
    "!python3 -m venv rag-pipeline -- quiet\n",
    "!source rag/bin/activate --quiet\n",
    "\n",
    "##### Install dependencies\n",
    "\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "docs0 = loader.load_data(file_path=Path(\"data/State of AI Report 2023.pdf\"), metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " docs is a <class 'list'>, of length 163, where each element is a <class 'llama_index.core.schema.Document'> object\n"
     ]
    }
   ],
   "source": [
    "print(f\" docs is a {type(docs0)}, of length {len(docs0)}, where each element is a {type(docs0[0])} object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_: 2904107e-66a6-4a84-9329-d672455b7d14\n",
      "embedding: None\n",
      "metadata: {'total_pages': 163, 'file_path': 'data/State of AI Report 2023.pdf', 'source': '95'}\n",
      "excluded_embed_metadata_keys: []\n",
      "excluded_llm_metadata_keys: []\n",
      "relationships: {}\n",
      "text:     In Oct 2022, Shutterstock - a leading stock multimedia provider - announced it will work with OpenAI to bring \n",
      "DALL·E-powered content onto the platform. Then in July 2023, the two companies signed a 6-year content \n",
      "licensing agreement that would give OpenAI access to Shutterstock's image, video and music libraries and \n",
      "associated metadata for model training. Furthermore, Shutterstock will offer its customers indemniﬁcation for AI \n",
      "image creation. The company also entered into a content license with Meta for GenAI. This pro-GenAI stance is in \n",
      "stark contrast to Shutterstock’s competitor, Getty Images, which is profoundly against GenAI as evidenced by its \n",
      "ongoing lawsuit against Stability AI for copyright infringement ﬁled in Feb 2023. \n",
      "stateof.ai 2023\n",
      "#stateofai | 95\n",
      " Introduction | Research | Industry | Politics | Safety | Predictions\n",
      "2022 Prediction: A major user generated content site negotiates a commercial \n",
      "settlement with a start-up producing AI models (e.g. OpenAI) for training on their corpus\n",
      "vs.\n",
      "\n",
      "start_char_idx: None\n",
      "end_char_idx: None\n",
      "text_template: {metadata_str}\n",
      "\n",
      "{content}\n",
      "metadata_template: {key}: {value}\n",
      "metadata_seperator: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in docs0[94]:\n",
    "    print (f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text and add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_slide_text(text:str) -> str: \n",
    "    \"\"\"\n",
    "    Cleans the provided slide by removing specific patterns and extra whitespace. \n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    Returns: \n",
    "    \"\"\"\n",
    "    # Remove the footer text\n",
    "    text = text.replace(\"stateof.ai 2023\", \"\")\n",
    "\n",
    "    # Remove the header text\n",
    "    text = text.replace(\"Introduction | Research | Industry | Politics | Safety | Predictions\", \"\")\n",
    "\n",
    "    # Remove the pattern \"#stateofai | n\"\n",
    "    text = re.sub(r\"#stateofai(\\s*\\|\\s*\\d+)?\", \"\", text)\n",
    "\n",
    "    # Replace multiple consecutive spaces with a single space\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "\n",
    "    # Remove any leading or trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_section(document):\n",
    "    \"\"\"\n",
    "    Assigns a section to the document based on its page number.\n",
    "\n",
    "    The function updates the 'metadata' attribute of the document with a key 'section'\n",
    "    that has a value corresponding to the section the page number falls into.\n",
    "\n",
    "    Sections:\n",
    "    - Page 1 through 10: Introduction\n",
    "    - Page 11 through 68: Research\n",
    "    - Page 69 through 120: Politics\n",
    "    - Page 121 through 137: Safety\n",
    "    - Pages 138 and beyond: Predictions\n",
    "\n",
    "    Args:\n",
    "    - document (Document): The Document object to be updated.\n",
    "\n",
    "    Returns:\n",
    "    None. The function updates the Document object in-place.\n",
    "    \"\"\"\n",
    "\n",
    "    page_number = int(document.metadata['source'])\n",
    "\n",
    "    if 1 <= page_number <= 10:\n",
    "        document.metadata['section'] = 'Introduction'\n",
    "    elif 11 <= page_number <= 68:\n",
    "        document.metadata['section'] = 'Research'\n",
    "    elif 69 <= page_number <= 120:\n",
    "        document.metadata['section'] = 'Politics'\n",
    "    elif 121 <= page_number <= 137:\n",
    "        document.metadata['section'] = 'Safety'\n",
    "    else:\n",
    "        document.metadata['section'] = 'Predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Oct 2022, Shutterstock - a leading stock multimedia provider - announced it will work with OpenAI to bring \n",
      "DALL·E-powered content onto the platform. Then in July 2023, the two companies signed a 6-year content \n",
      "licensing agreement that would give OpenAI access to Shutterstock's image, video and music libraries and \n",
      "associated metadata for model training. Furthermore, Shutterstock will offer its customers indemniﬁcation for AI \n",
      "image creation. The company also entered into a content license with Meta for GenAI. This pro-GenAI stance is in \n",
      "stark contrast to Shutterstock’s competitor, Getty Images, which is profoundly against GenAI as evidenced by its \n",
      "ongoing lawsuit against Stability AI for copyright infringement ﬁled in Feb 2023. \n",
      "\n",
      "\n",
      " \n",
      "2022 Prediction: A major user generated content site negotiates a commercial \n",
      "settlement with a start-up producing AI models (e.g. OpenAI) for training on their corpus\n",
      "vs.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each Document object in docs0\n",
    "for doc in docs0:\n",
    "    # Update the metadata using assign_section\n",
    "    assign_section(doc)\n",
    "\n",
    "    # Metadata keys that are excluded from text for the embed model.\n",
    "    doc.excluded_embed_metadata_keys=['file_name']\n",
    "\n",
    "    # Apply clean_slide_text to the text attribute1\n",
    "    doc.text = clean_slide_text(doc.text)\n",
    "    \n",
    "print (docs0[94].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_pages': 163,\n",
       " 'file_path': 'data/State of AI Report 2023.pdf',\n",
       " 'source': '95',\n",
       " 'section': 'Politics'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"In Oct 2022, Shutterstock - a leading stock multimedia provider - announced it will work with OpenAI to bring \\nDALL·E-powered content onto the platform. Then in July 2023, the two companies signed a 6-year content \\nlicensing agreement that would give OpenAI access to Shutterstock's image, video and music libraries and \\nassociated metadata for model training. Furthermore, Shutterstock will offer its customers indemniﬁcation for AI \\nimage creation. The company also entered into a content license with Meta for GenAI. This pro-GenAI stance is in \\nstark contrast to Shutterstock’s competitor, Getty Images, which is profoundly against GenAI as evidenced by its \\nongoing lawsuit against Stability AI for copyright infringement ﬁled in Feb 2023. \\n\\n\\n \\n2022 Prediction: A major user generated content site negotiates a commercial \\nsettlement with a start-up producing AI models (e.g. OpenAI) for training on their corpus\\nvs.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs0[94].metadata\n",
    "\n",
    "docs0[94].get_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategies: Options\n",
    "\n",
    "Two options here: \n",
    "1. Directly send the entire Document object to the index\n",
    "    - Maintains entire document as a single unit \n",
    "    - Useful when documents are relatively short and contexts between different parts of the document is important \n",
    "2. Covert the Document into Node objects before sending them to the index\n",
    "    - Practical when the documents are long and require breaking down into chunks (or nodes) before indexing\n",
    "    - Useful to retrieve specific parts of a document than the entire document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Document object to Node: Node and NodeParser\n",
    "\n",
    "- A Node represents a chunk of a source document \n",
    "- Node contain metadata and relationship information with other nodes\n",
    "- Nodes are first-class citizens in LlamaIndex, this means Nodes and their attributes can be defined directly\n",
    "- Every node derived from a Document will inherit the same metadata from that Document\n",
    "- Alternatively, we can parse source Documents into Nodes using the NodeParser classes. \n",
    "\n",
    "\n",
    "**Chunk Size:** \n",
    "\n",
    "Choosing the optimal chunk_size provides optimal results \n",
    "- Smaller chunk_size provides granular chunks, but we risk that the essential information might not be be among the top retrived chunks\n",
    "- Larger chunk size might contain all necessary infromation within the top chunks \n",
    "- Increase in chunk size directs more information into the LLM. This ensures a comprehensive context but might slow down the system. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count for a slide: 128.12269938650306\n",
      "Average word count per bullet point: 9.252131000448632\n",
      "Longest bullet point: 28\n",
      "Average word count in a section: 4176.80\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for bullet points and newlines\n",
    "split_pattern = r\"\\n●|\\n-|\\n\"\n",
    "\n",
    "# Initialize lists to store the word counts of all chunks and entire texts across all documents\n",
    "chunk_word_counts = []\n",
    "entire_text_word_counts = []\n",
    "\n",
    "# Initialize a dictionary to store word counts and slide counts by section\n",
    "section_data = {}\n",
    "\n",
    "# Iterate through each Document object in your list of documents\n",
    "for doc in docs0:\n",
    "    # Split the document's text into chunks based on the pattern\n",
    "    chunks = re.split(split_pattern, doc.text)\n",
    "\n",
    "    # Calculate the number of words in each chunk and store it\n",
    "    chunk_word_counts.extend([len(chunk.split()) for chunk in chunks])\n",
    "\n",
    "    # Calculate the number of words in the entire text and store it\n",
    "    entire_word_count = len(doc.text.split())\n",
    "    entire_text_word_counts.append(entire_word_count)\n",
    "\n",
    "    # Update the word count and slide count for the section in the dictionary\n",
    "    section = doc.metadata['section']\n",
    "    if section in section_data:\n",
    "        section_data[section]['word_count'] += entire_word_count\n",
    "        section_data[section]['slide_count'] += 1\n",
    "    else:\n",
    "        section_data[section] = {'word_count': entire_word_count, 'slide_count': 1}\n",
    "\n",
    "# Calculate the total word count across all sections\n",
    "total_word_count = sum(data['word_count'] for data in section_data.values())\n",
    "\n",
    "# Calculate the number of sections\n",
    "num_sections = len(section_data)\n",
    "\n",
    "# Calculate the average word count across all sections\n",
    "average_word_count_across_sections = total_word_count / num_sections\n",
    "\n",
    "# Calculate summary statistics for chunks\n",
    "average_chunk_word_count = sum(chunk_word_counts) / len(chunk_word_counts)\n",
    "max_chunk_word_count = max(chunk_word_counts)\n",
    "\n",
    "# Calculate average word count for entire texts\n",
    "average_entire_text_word_count = sum(entire_text_word_counts) / len(entire_text_word_counts)\n",
    "\n",
    "print(f\"Average word count for a slide: {average_entire_text_word_count}\")\n",
    "print(f\"Average word count per bullet point: {average_chunk_word_count}\")\n",
    "print(f\"Longest bullet point: {max_chunk_word_count}\")\n",
    "print(f\"Average word count in a section: {average_word_count_across_sections:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Strategy\n",
    "\n",
    "- *NodeParsers* are a simple abstraction that take a list of documents and chunk them into Node objects. \n",
    "Each *Node* is a specific chunk of the parent document.\n",
    "- Strategy: Utilize smaller child chunks that refer to bigger parent chunks.\n",
    "    - Use *SimpleNodeParser* with a *SentenceSplitter* to create \"base nodes\" aka parent chunks\n",
    "    - Use *SentenceWindowNodeParser* to create child nodes that represent bullet points in the slide deck along with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from pathlib import Path\n",
    "\n",
    "# bullet_splitter = SentenceSplitter(paragraph_separator=r\"\\n●|\\n-|\\n\", chunk_size=250)\n",
    "\n",
    "\n",
    "parser = SentenceSplitter.from_defaults(\n",
    "                chunk_size=250,\n",
    "                paragraph_separator=r\"\\n●|\\n-|\\n\",\n",
    "                include_metadata=True,\n",
    "                include_prev_next_rel=True)\n",
    "\n",
    "slides_nodes = parser.get_nodes_from_documents(docs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='d7f2cfb4-e053-4e58-b7d7-6358336c7035', embedding=None, metadata={'total_pages': 163, 'file_path': 'data/State of AI Report 2023.pdf', 'source': '29', 'section': 'Research'}, excluded_embed_metadata_keys=['file_name'], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77577cc4-2cce-48be-99ed-7bbde7d42608', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 163, 'file_path': 'data/State of AI Report 2023.pdf', 'source': '29', 'section': 'Research'}, hash='3b856327adfb9de7690d0e9932df2c2cba0c5eb4c90e04e9420ff8e9b93116c5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4cfc1444-1007-483b-b020-b457cd07fc6b', node_type=<ObjectType.TEXT: '1'>, metadata={'total_pages': 163, 'file_path': 'data/State of AI Report 2023.pdf', 'source': '29', 'section': 'Research'}, hash='bdb611ea7014bf3d76388dfd64916a160c3e5d703ec6633ee60f9ffa751df940'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b5047b2a-949b-4f00-8b5b-3029521f6db2', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6d961776576e96a05346bfd04b01257fbbf81eb5bcc8bffd3352a345afe576d5')}, text='We’re nowhere near a deﬁnitive answer: Synthetic data is becoming more helpful, but \\nthere is still evidence showing that in some cases generated data makes models forget.\\n\\n● Despite the seemingly inﬁnitely proprietary and publicly available data, the largest models are actually running \\nout of data to train on, and testing the limits of scaling laws. One way to alleviate this problem (which has been \\nextensively explored in the past) is to train on AI-generated data, whose volume is only bounded by compute.\\n\\nBreaking the data ceiling: AI-generated content\\n \\n● Researchers from Google ﬁne-tune the Imagen text-to-image model for \\nclass-conditional ImageNet, then generated one to 12 synthetic versions \\nof ImageNet on which they trained their models (in addition to the \\noriginal ImageNet). They showed that increasing the size of the synthetic \\ndataset monotonically improved the model’s accuracy.\\n● Other researchers showed that the compounding errors from training on \\nsynthetic text online may result in model collapse, “where generated data \\nend up polluting the training set of the next generation of models”.', start_char_idx=134, end_char_idx=1255, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1121"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'We’re nowhere near a deﬁnitive answer: Synthetic data is becoming more helpful, but \\nthere is still evidence showing that in some cases generated data makes models forget.\\n\\n● Despite the seemingly inﬁnitely proprietary and publicly available data, the largest models are actually running \\nout of data to train on, and testing the limits of scaling laws. One way to alleviate this problem (which has been \\nextensively explored in the past) is to train on AI-generated data, whose volume is only bounded by compute.\\n\\nBreaking the data ceiling: AI-generated content\\n \\n● Researchers from Google ﬁne-tune the Imagen text-to-image model for \\nclass-conditional ImageNet, then generated one to 12 synthetic versions \\nof ImageNet on which they trained their models (in addition to the \\noriginal ImageNet). They showed that increasing the size of the synthetic \\ndataset monotonically improved the model’s accuracy.\\n● Other researchers showed that the compounding errors from training on \\nsynthetic text online may result in model collapse, “where generated data \\nend up polluting the training set of the next generation of models”.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slides_nodes[94]\n",
    "len(slides_nodes[94].text)\n",
    "slides_nodes[94].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceWindowNodeParser\n",
    "\n",
    "The Sentence Window Node Parser parses the sentences in the document into Nodes and captures a window of surrounding sentences for each node. Understanding the context of a sentence can provide valuable insights. \n",
    "\n",
    "Each node contains a window of surrounding sentences in its metadata. The window size is defined by the window_size attribute, which defaults to 3. This means that for each sentence, the parser will include the 3 sentences before and after it in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "def custom_sentence_splitter(text: str) -> List[str]:\n",
    "    return re.split(r'\\n●|\\n-|\\n', text)\n",
    "\n",
    "bullet_node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    sentence_splitter=custom_sentence_splitter,\n",
    "    window_size=3,\n",
    "    include_prev_next_rel=True,\n",
    "    include_metadata=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexNode in LlamaIndex\n",
    "\n",
    "An IndexNode is a node object used in LlamaIndex. The Index is a data structure that allows for quick retrieval of relevant context for a user query, which is fundamental for retrieval-augmented generation (RAG) use cases. At its core, the IndexNode inherits properties from a TextNode, meaning it primarily represents textual content.\n",
    "\n",
    "Every IndexNode has an index_id attribute. This index_id acts as a unique identifier or reference to another object, allowing the node to point or link to other entities within the system, providing a layer of connectivity on top of the textual content. Connected chunks allow for more context for synthesis. \n",
    "\n",
    "IndexNode inherits its textual content from the TextNode and serves as a pointer for other entities in the system, allowing nodes to represent both content and relationships to other objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "sub_node_parsers =[bullet_node_parser]\n",
    "\n",
    "all_nodes = []\n",
    "\n",
    "for base_node in slides_nodes:\n",
    "    for parser in sub_node_parsers:\n",
    "        sub_nodes = parser.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model and LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302410d088ef49f6a2dbca2ff6d55cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a260e6b76ab427da2ee951867ab845f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892d805cb3dc40599b1e7ba5415e411b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86208843d2c4a5c9ef9f43feda35476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f3381f1f984248b8630094e3bd1ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255b878ff5ec4a77a39153aeb0ffde57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "\n",
    "embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new prompt template\n",
    "template = \"\"\"Below is context that has been retrieved. Your task is to synthesize \\\n",
    "\n",
    "the query, which is delimited by triple backticks,  and write a response that appropriately answers the query based on the retrieved context.\n",
    "\n",
    "### Query:\n",
    "```{query_str}```\n",
    "\n",
    "### Response:\n",
    "\n",
    "Begin!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"Deci/DeciLM-6b-instruct\",\n",
    "    tokenizer_name=\"Deci/DeciLM-6b-instruct\",\n",
    "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "    # query_wrapper_prompt=PromptTemplate(template),\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={'trust_remote_code':True},\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": False},\n",
    "    # generate_kwargs={\"temperature\": 0.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and VectorStoreIndex \n",
    "\n",
    "A VectorStoreIndex in LlamaIndex is a type of index that uses vector representations of text for efficient retrieval of relevant context. The VectorStoreIndex takes as input the IndexNode objects and used the specified embedding model to convert the text content of these nodes into vector representations. These vectors are then stored in the VectorStore. \n",
    "\n",
    "For a given query, the VectorStoreIndex converts the query into a vector using the same embedding model and retrieves the most relevant nodes from the VectorStore for the query using the nearest neighbor search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index_chunk = VectorStoreIndex(all_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever object\n",
    "\n",
    "A retriever is a component that is responsible for fetching relevant context from the index given a user query. When you call as_retriever on a VectorStoreIndex, it returns a VectorStoreRetriever object. \n",
    "\n",
    "The RecursiveRetriever is can handle complex retrieval task by exploring links between nodes, fetching data from connected retrievers or query engines recursively. It efficiently retrieves information from various sources, including IndexNodes, consolidating results from multiple sources to provide comprehensive responses for complex retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "\n",
    "\n",
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The query can either be a simple string or a structured *QueryBundle* object. \n",
    "- The *Retrieve* method accepts the query, converts it into a *QueryBundle* and calls an internal method to fetch a list of nodes based on this query. \n",
    "- Each node in the list has a confidence score in relation to the query. \n",
    "- *display_source_node* accepts a *NodeWithScore* object, consisting of nodeID, its similarity score and a truncated version of its content. \n",
    "- The retrieved text is displayed in a concise manner to the specified *source_length*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: What is FlashAttention?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 10e7daff-aebb-4d6c-aecf-55411b1a905d\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 10e7daff-aebb-4d6c-aecf-55411b1a905d: What is FlashAttention?\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 10e7daff-aebb-4d6c-aecf-55411b1a905d<br>**Similarity:** 0.677945985196128<br>**Text:** ● FlashAttention introduces a signiﬁcant memory saving by making attention linear instead of quadratic in \n",
       "sequence length. FlashAttention-2 further improves computing the attention matrix by having fewer non-matmul \n",
       "FLOPS, better parallelism and better work partitioning. The result is a 2.8x training speedup of GPT-style models.\n",
       "● Reducing the number of bits in the parameters reduces both the memory footprint and the latency of LLMs. The \n",
       "case for 4-bit precision: k-bit Inference Scaling Laws shows across a variety of LLMs that 4-bit quantisation is \n",
       "universally optimal for maximizing zero-shot accuracy and reducing the number of bits used.\n",
       "● Speculative decoding enables decoding multiple tokens in parallel through multiple model heads rather than \n",
       "forward passes, speeding up inference by 2-3X for certain models.\n",
       "● SWARM Parallelism is a training algorithm designed for poorly connected and unreliable devices.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "\n",
    "nodes = retriever_chunk.retrieve(\n",
    "    \"What is FlashAttention?\"\n",
    ")\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrieverQueryEngine \n",
    "\n",
    "- The RetrieverQueryEngine takes a retriever and a response synthesizer as inputs. \n",
    "- The retriever is responsible for fetching relevant IndexNode objects from the index, \n",
    "- The response synthesizer is used to generate a natural language response based on the retrieved nodes and the user query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
    "    retriever_chunk,\n",
    "    verbose=True,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the indexed PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "   \"Who are the authors of this report?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "   \"What is new about FlashAttention?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "    \"Does the report mention anything about inference and latency concerns?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "    \"What does the report say about text to image models?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "    \"Summarize the research section of the report\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine_chunk.query(\n",
    "    \"What does the report say about the importance of quality prompts?\"\n",
    ")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://docs.llamaindex.ai/en/stable/\n",
    "2. https://deci.ai/blog/rag-with-llamaindex-and-decilm-a-step-by-step-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
