{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What are RAG Rerankers?**\n",
    "- Definition of RAG Rerankers\n",
    "- Introduction to the RAG (Retrieval-Augmented Generation) framework\n",
    "- Overview of reranking process in natural language generation\n",
    "\n",
    "**Why Use RAG Rerankers?**\n",
    "- Improving answer relevance\n",
    "- Enhancing response diversity\n",
    "- Increasing model performance and user satisfaction\n",
    "\n",
    "**How Do RAG Rerankers Work?**\n",
    "- Retrieval phase: obtaining relevant passages/documents\n",
    "- Augmentation phase: encoding retrieved content with the prompt\n",
    "- Generation phase: reranking generated responses based on relevance scores\n",
    "- Fine-tuning and optimization techniques\n",
    "\n",
    "**Implementing RAG Rerankers**\n",
    "- Choosing appropriate retrieval methods\n",
    "- Integrating RAG rerankers with existing NLP pipelines\n",
    "- Selecting relevant prompts and inputs\n",
    "- Fine-tuning and hyperparameter tuning\n",
    "\n",
    "**Benefits of RAG Rerankers**\n",
    "- Improved answer quality and relevance\n",
    "- Enhanced user experience\n",
    "- Better performance on specific tasks\n",
    "- Potential for domain adaptation and transfer learning\n",
    "\n",
    "**Challenges and Considerations**\n",
    "- Computational complexity and resource requirements\n",
    "- Fine-tuning data availability and quality\n",
    "- Overfitting and generalization issues\n",
    "- Ethical considerations and biases in retrieval and reranking\n",
    "\n",
    "**Applications of RAG Rerankers**\n",
    "- Question answering systems\n",
    "- Chatbots and virtual assistants\n",
    "- Information retrieval and summarization\n",
    "- Content recommendation systems\n",
    "\n",
    "**Conclusion**\n",
    "- Recap of the importance and functionality of RAG rerankers\n",
    "- Future directions and research avenues\n",
    "- Final thoughts on the potential impact of RAG rerankers in NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **What?**\n",
    "\n",
    "**The problem:**\n",
    "\n",
    "Due to information loss in optimizing semantic search with vector representations of text documents, top results frequently fail to capture relevant information. How can we ensure that potentially useful but lower-ranked information is included in the results to improve the language model's response?\n",
    "\n",
    "**Introduction:** \n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) system combines information retrieval capabilities with the generative prowess of LLMs. While RAG is easy to use, it is hard to master. In out-of-the-box RAG, we put documents into a vector database and add an LLM on top in the expectation that it would provide us with the relevant results. \n",
    "\n",
    "However, this might not always work. \n",
    "- RAG performs semantic search across many text documents, ranging from tens to billions of documents. \n",
    "- Typically, we transform our text into vectors\n",
    "\n",
    "\n",
    "**Terminology:** \n",
    "\n",
    "- Context Window: The number of tokens the model can take as input when generating responses. Larger context windows improve LLM performance and their usefulness across various applications.\n",
    "\n",
    "- LLM recall: The ability of an LLM to find information from the text placed within its context window\n",
    "\n",
    "- Retrieval recall: The percentage of correct documents returned by the retriever in response to a query\n",
    "\n",
    "- Context stuffing: \n",
    "\n",
    "- Reranking: The \"learning-to-rank approach\" or simply the re-ranker component sifts through the responses procided by the retriever and ranks them based on their relevance. \n",
    "\n",
    "- Bi-encoder: There are two separate encoders - one for encoding the input query and another for encoding the candidate documents. These encoders work independently, producing embeddings for the query and each document. During inference, the model computes the similarity score between the query and each document independently and the document with the highest similarity score is considered the most relevant. Commonly used in tasks where document retrieval or ranking is primary goal, such as search engines or recommendation systems. \n",
    "\n",
    "- Cross-encoder: The query and document are processed together in a single encoder. The model takes both the query and teh document as input and produces a joint representation. There exists a single similarity score for each query-document pair and the document with the highest score is considered the most relevant. Commonly used in scenarios where understanding the context or relationship between the query and document is essential such as duplicate detection, question answering or information retrieval. \n",
    "\n",
    "<img src=\"./rerank.jpeg\" style=\"height: 300px;\"/>\n",
    "\n",
    "Two-stage Retrieval: \n",
    "\n",
    "1. First-stage model: (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. \n",
    "2. Second-stage model: the raranker is used to rerank those documents retrieved by the first-stage model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why?**\n",
    "\n",
    "- Because retrieving a small set of documents from a large dataset is much faster than reranking a large set of documents. \n",
    "- Rerankers are slow, and retrievers are fast. However, rerankers are much more accurate than embedding models. \n",
    "- Intuition behind the problem: Bi-encoders compress all of the possible meanings of a document into a single vector theeby losing information. These embeddings are created ahead of receiving the query and therefore no context exists between the document embeddings and the user query, resulting in sometimes irrelevant results. \n",
    "- On the other hand, a reranker can receive the raw information directly into the large transformer computation, meaning less information loss. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
